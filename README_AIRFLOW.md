# ğŸ¢ Pipeline ETL de Propiedades Inmobiliarias con Apache Airflow

## ğŸ“‹ DescripciÃ³n del Proyecto

Este proyecto implementa un pipeline ETL (Extract, Transform, Load) completo para la obtenciÃ³n y procesamiento automatizado de datos de propiedades inmobiliarias del mercado argentino. Utiliza Apache Airflow para orquestaciÃ³n y estÃ¡ diseÃ±ado para ejecutarse en un entorno controlado con Astro CLI.

## ğŸ¯ Problema a Abordar

### Contexto
El mercado inmobiliario argentino carece de datasets pÃºblicos actualizados y estructurados que permitan:
- AnÃ¡lisis de tendencias de precios
- Estudios de mercado por zona geogrÃ¡fica
- Desarrollo de modelos predictivos
- InvestigaciÃ³n acadÃ©mica y comercial

### SoluciÃ³n
Pipeline automatizado que genera diariamente un dataset limpio y estructurado con:
- **10,000+ propiedades** por ejecuciÃ³n
- **Datos normalizados** y validados
- **MÃºltiples formatos** de salida
- **Reportes de calidad** automÃ¡ticos
- **Metadatos completos** del dataset

## ğŸ“Š Fuente de Datos

### ArgentProp (www.argenprop.com)
- **Tipo**: Portal inmobiliario lÃ­der en Argentina
- **Cobertura**: +100,000 propiedades activas
- **UbicaciÃ³n**: Capital Federal y Gran Buenos Aires
- **ActualizaciÃ³n**: Datos en tiempo real
- **CategorÃ­as**: Departamentos, casas, oficinas, terrenos

### MetodologÃ­a de ExtracciÃ³n
- **Web Scraping Ã©tico** con rate limiting
- **RotaciÃ³n de User-Agents** para evitar bloqueos
- **Manejo robusto de errores** y reintentos
- **Respeto por robots.txt** y tÃ©rminos de servicio

## ğŸ—ï¸ Arquitectura del Pipeline

### TecnologÃ­as Utilizadas
- **Apache Airflow 2.7+**: OrquestaciÃ³n y scheduling
- **Astro CLI**: DistribuciÃ³n y deployment
- **Python 3.9+**: Lenguaje principal
- **pandas**: ManipulaciÃ³n de datos
- **BeautifulSoup4**: Web scraping
- **Parquet**: Almacenamiento eficiente

### Estructura del DAG

```mermaid
graph TD
    A[Start Pipeline] --> B[Prepare Environment]
    B --> C[Extract Properties]
    C --> D[Transform Data]
    D --> E[Load Dataset]
    E --> F[Validate Dataset]
    F --> G[Generate Report]
    G --> H[End Pipeline]
```

## ğŸ“ Estructura del Proyecto

```
Props-Scraper/
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ propiedades_etl_dag.py          # DAG principal de Airflow
â”œâ”€â”€ airflow_utils/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ extraction.py                   # MÃ³dulo de extracciÃ³n
â”‚   â”œâ”€â”€ transformation.py               # MÃ³dulo de transformaciÃ³n
â”‚   â””â”€â”€ loading.py                      # MÃ³dulo de carga
â”œâ”€â”€ data/                               # Directorio de outputs
â”œâ”€â”€ Dockerfile                          # ConfiguraciÃ³n Docker para Astro
â”œâ”€â”€ astro.yaml                          # ConfiguraciÃ³n Astro CLI
â”œâ”€â”€ requirements.txt                    # Dependencias Python
â”œâ”€â”€ test_pipeline_local.py              # Script de pruebas local
â””â”€â”€ README_AIRFLOW.md                   # Esta documentaciÃ³n
```

## ğŸ”„ Etapas del Pipeline

### 1. ğŸ“¥ ExtracciÃ³n (Extract)
**Archivo**: `airflow_utils/extraction.py`

- **Input**: ParÃ¡metros de configuraciÃ³n
- **Proceso**:
  - ConstrucciÃ³n de URLs de bÃºsqueda
  - Web scraping con control de rate limiting
  - ExtracciÃ³n de metadatos de propiedades
  - Manejo de errores y reintentos
- **Output**: Archivo JSON con datos crudos

**Datos extraÃ­dos por propiedad**:
- TÃ­tulo y descripciÃ³n
- Precio publicado
- UbicaciÃ³n completa
- CaracterÃ­sticas (habitaciones, baÃ±os, superficie)
- URL y enlaces
- ImÃ¡genes
- Timestamp de scraping

### 2. ğŸ”„ TransformaciÃ³n (Transform)
**Archivo**: `airflow_utils/transformation.py`

- **Input**: Datos crudos en JSON
- **Proceso**:
  - Limpieza de precios y conversiÃ³n a valores numÃ©ricos
  - NormalizaciÃ³n de ubicaciones (barrio, zona, ciudad)
  - ExtracciÃ³n de caracterÃ­sticas numÃ©ricas
  - CÃ¡lculo de scores de calidad de datos
  - ValidaciÃ³n y detecciÃ³n de anomalÃ­as
- **Output**: DataFrame optimizado en formato Parquet

**Transformaciones aplicadas**:
- **Precios**: USD 150.000 â†’ 150000.0 (float)
- **Ubicaciones**: "Palermo, Capital Federal" â†’ {barrio: "Palermo", ciudad: "Capital Federal"}
- **Habitaciones**: "3 amb" â†’ 3 (int)
- **Superficie**: "85 mÂ²" â†’ 85.0 (float)

### 3. ğŸ’¾ Carga (Load)
**Archivo**: `airflow_utils/loading.py`

- **Input**: Datos transformados
- **Proceso**:
  - GeneraciÃ³n de dataset en mÃºltiples formatos
  - CreaciÃ³n de reportes estadÃ­sticos
  - GeneraciÃ³n de metadatos automÃ¡ticos
  - ValidaciÃ³n de integridad
- **Output**: Dataset completo multi-formato

**Formatos generados**:
- **Excel**: Con hojas de estadÃ­sticas y calidad
- **CSV**: Para compatibilidad universal
- **Parquet**: Para anÃ¡lisis big data
- **JSON**: Para integraciÃ³n con APIs
- **Metadata**: DocumentaciÃ³n del dataset

### 4. âœ… ValidaciÃ³n
- VerificaciÃ³n de completitud de datos
- ValidaciÃ³n de calidad mÃ­nima
- ComprobaciÃ³n de archivos generados
- GeneraciÃ³n de reportes de Ã©xito/fallo

## ğŸ“Š Calidad de Datos

### Score de Calidad (0-100)
Cada propiedad recibe un score basado en:
- **Precio vÃ¡lido** (30 puntos)
- **UbicaciÃ³n completa** (25 puntos)
- **CaracterÃ­sticas fÃ­sicas** (25 puntos)
- **Enlaces vÃ¡lidos** (20 puntos)

### Niveles de Calidad
- **Alta** (80-100): Datos completos y confiables
- **Media** (50-79): Datos parciales pero Ãºtiles
- **Baja** (0-49): Datos incompletos

### MÃ©tricas de Completitud
- Porcentaje de campos completos por columna
- DistribuciÃ³n de scores de calidad
- DetecciÃ³n de outliers y anomalÃ­as

## ğŸš€ EjecuciÃ³n del Pipeline

### Usando Astro CLI (Recomendado)

```bash
# 1. Instalar Astro CLI
curl -sSL install.astronomer.io | sudo bash

# 2. Inicializar proyecto
cd Props-Scraper
astro dev init

# 3. Iniciar Airflow localmente
astro dev start

# 4. Acceder a la UI
# http://localhost:8080
# User: admin / Password: admin

# 5. Activar el DAG 'propiedades_etl_pipeline'
```

### Prueba Local (Desarrollo)

```bash
# Ejecutar pipeline completo sin Airflow
python test_pipeline_local.py

# Opciones disponibles:
# 1. Prueba completa del pipeline
# 2. Prueba de componentes individuales  
# 3. Ambas
```

### ConfiguraciÃ³n del DAG

```python
# ParÃ¡metros modificables en el DAG
SCRAPING_CONFIG = {
    'property_type': 'departamentos',    # departamentos, casas
    'operation_type': 'venta',           # venta, alquiler
    'location': 'capital-federal',       # zona geogrÃ¡fica
    'price_range_from': 50000,           # precio mÃ­nimo USD
    'price_range_to': 300000,            # precio mÃ¡ximo USD
    'currency': 'dolares',               # dolares, pesos
    'max_pages': 10,                     # pÃ¡ginas a scrapear
    'sort_by': 'masnuevos'              # criterio de orden
}
```

## ğŸ“ˆ Resultados Esperados

### Dataset TÃ­pico
- **Registros**: 8,000 - 12,000 propiedades
- **Completitud**: 85%+ de datos vÃ¡lidos
- **Calidad promedio**: 70/100
- **Cobertura**: 50+ barrios de Capital Federal
- **Formatos**: 5 archivos de salida + metadatos

### Tiempo de EjecuciÃ³n
- **ExtracciÃ³n**: 15-25 minutos
- **TransformaciÃ³n**: 2-3 minutos
- **Carga**: 1-2 minutos
- **Total**: ~25-30 minutos

### TamaÃ±o de Archivos
- **Excel**: ~15-20 MB
- **CSV**: ~8-12 MB
- **Parquet**: ~3-5 MB
- **JSON**: ~25-30 MB

## ğŸ” Monitoreo y Alertas

### Logs Detallados
- Progreso de extracciÃ³n por pÃ¡gina
- EstadÃ­sticas de transformaciÃ³n
- MÃ©tricas de calidad en tiempo real
- Errores y warnings detallados

### Validaciones AutomÃ¡ticas
- MÃ­nimo de registros extraÃ­dos
- Calidad promedio aceptable
- Existencia de archivos de salida
- Integridad de datos crÃ­ticos

### Alertas de Fallo
- Notificaciones por email (configurable)
- Logs de error detallados
- Reintentos automÃ¡ticos
- Fallback a modos de emergencia

## ğŸ“Š Casos de Uso del Dataset

### AnÃ¡lisis de Mercado
- Trends de precios por barrio
- AnÃ¡lisis de oferta y demanda
- Comparativas temporales
- Reportes ejecutivos

### Machine Learning
- Modelos predictivos de precios
- ClasificaciÃ³n de propiedades
- DetecciÃ³n de oportunidades
- AnÃ¡lisis de clusters geogrÃ¡ficos

### Research AcadÃ©mico
- Estudios de gentrificaciÃ³n
- AnÃ¡lisis socioeconÃ³micos
- InvestigaciÃ³n urbana
- Tesis y publicaciones

### Desarrollo de Productos
- Apps de valuaciÃ³n automÃ¡tica
- Dashboards en tiempo real
- APIs de datos inmobiliarios
- Herramientas de inversiÃ³n

## ğŸ› ï¸ Decisiones de DiseÃ±o

### Â¿Por quÃ© Airflow?
- **OrquestaciÃ³n robusta** de pipelines complejos
- **Scheduling avanzado** con mÃºltiples triggers
- **Monitoreo visual** del estado de ejecuciÃ³n
- **Escalabilidad** para crecer el proyecto
- **Ecosystem** rico de operadores y conectores

### Â¿Por quÃ© Web Scraping?
- **Datos pÃºblicos** disponibles en tiempo real
- **No hay APIs** oficiales del mercado inmobiliario
- **Cobertura completa** del mercado
- **Costo cero** vs. servicios pagos

### Â¿Por quÃ© mÃºltiples formatos?
- **Excel**: AnÃ¡lisis manual y presentaciones
- **CSV**: Compatibilidad universal
- **Parquet**: Eficiencia para big data
- **JSON**: IntegraciÃ³n con APIs y web apps

### Â¿Por quÃ© scores de calidad?
- **Filtrado inteligente** de datos incompletos
- **PriorizaciÃ³n** de registros confiables
- **MÃ©tricas** para mejorar el pipeline
- **Transparencia** en la calidad del dataset

## ğŸ”§ Mantenimiento

### Actualizaciones del Pipeline
- Monitoreo de cambios en el sitio web
- Ajustes de selectores CSS
- OptimizaciÃ³n de performance
- Mejoras en calidad de datos

### Escalabilidad
- ParalelizaciÃ³n de tareas
- DistribuciÃ³n en mÃºltiples workers
- Caching de resultados intermedios
- OptimizaciÃ³n de memoria

## ğŸ“‹ Entrega de la Primera Fase

### Requisitos Cumplidos âœ…

1. **DAG funcional**: `dags/propiedades_etl_dag.py`
2. **Ejecutable en Astro CLI**: ConfiguraciÃ³n completa incluida
3. **Proceso completo ETL**: ExtracciÃ³n â†’ TransformaciÃ³n â†’ Carga
4. **Dataset reproducible**: Mismos parÃ¡metros = mismos resultados
5. **Ejecutable en vivo**: Demos y logs disponibles
6. **DocumentaciÃ³n completa**: Problema, fuente, decisiones

### Archivos de Entrega

```
Props-Scraper/
â”œâ”€â”€ dags/propiedades_etl_dag.py         # DAG principal â­
â”œâ”€â”€ airflow_utils/                      # MÃ³dulos ETL â­
â”œâ”€â”€ test_pipeline_local.py              # Pruebas locales â­
â”œâ”€â”€ Dockerfile                          # Config Docker â­
â”œâ”€â”€ astro.yaml                          # Config Astro â­
â”œâ”€â”€ requirements.txt                    # Dependencias â­
â””â”€â”€ README_AIRFLOW.md                   # DocumentaciÃ³n â­
```

### Demo en Vivo
1. **Inicio de Astro**: `astro dev start`
2. **ActivaciÃ³n del DAG** en la UI
3. **Monitoreo en tiempo real** del progreso
4. **VerificaciÃ³n de outputs** generados
5. **RevisiÃ³n de logs** detallados

---

**ğŸ¯ Este pipeline estÃ¡ listo para generar datasets de propiedades inmobiliarias de forma automatizada, reproducible y escalable, sentando las bases sÃ³lidas para todo el proyecto de anÃ¡lisis de datos.**
