"""
DAG de Airflow para el scraping y procesamiento de propiedades inmobiliarias

Este DAG implementa un pipeline completo ETL (Extract, Transform, Load) para:
1. Extraer datos de propiedades desde ArgentProp
2. Transformar y limpiar los datos 
3. Cargar el dataset final en m√∫ltiples formatos
4. Validar la calidad del dataset generado

Autor: Props Scraper Team
Fecha: Septiembre 2025
"""

from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.dummy import DummyOperator
from airflow.operators.bash import BashOperator
from airflow.utils.dates import days_ago

# Importar funciones de utilidades
import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from airflow_utils.extraction import extract_properties_data
from airflow_utils.transformation import transform_properties_data
from airflow_utils.loading import load_final_dataset, validate_final_dataset

# Configuraci√≥n del DAG
default_args = {
    'owner': 'props-scraper-team',
    'depends_on_past': False,
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 2,
    'retry_delay': timedelta(minutes=5),
    'start_date': days_ago(1),
}

# Par√°metros de configuraci√≥n para el scraping
SCRAPING_CONFIG = {
    'property_type': 'departamentos',
    'operation_type': 'venta', 
    'location': 'capital-federal',
    'price_range_from': 50000,
    'price_range_to': 300000,
    'currency': 'dolares',
    'max_pages': 10,
    'sort_by': 'masnuevos'
}

# Definici√≥n del DAG
dag = DAG(
    'propiedades_etl_pipeline',
    default_args=default_args,
    description='Pipeline ETL completo para datos de propiedades inmobiliarias',
    schedule_interval='@daily',  # Ejecutar diariamente
    catchup=False,
    max_active_runs=1,
    tags=['real-estate', 'etl', 'scraping', 'propiedades'],
    params=SCRAPING_CONFIG,
)

# Documentaci√≥n del DAG
dag.doc_md = """
# Pipeline ETL de Propiedades Inmobiliarias

## Descripci√≥n del Problema
Este DAG aborda la necesidad de obtener datos actualizados del mercado inmobiliario argentino 
para an√°lisis de precios, tendencias y caracter√≠sticas de propiedades. Los datos se extraen 
de ArgentProp, uno de los portales inmobiliarios m√°s importantes de Argentina.

## Fuente de Datos
- **Sitio web**: ArgentProp (www.argenprop.com)
- **M√©todo**: Web scraping automatizado
- **Tipo de datos**: Propiedades en venta (departamentos principalmente)
- **Ubicaci√≥n**: Capital Federal, Argentina
- **Actualizaci√≥n**: Diaria

## Arquitectura del Pipeline

### 1. Extracci√≥n (Extract)
- Scraping web de ArgentProp
- Extracci√≥n de metadatos de propiedades
- Manejo de rate limiting y errores
- Almacenamiento temporal en JSON

### 2. Transformaci√≥n (Transform)
- Limpieza y normalizaci√≥n de datos
- Extracci√≥n de precios num√©ricos
- Geolocalizaci√≥n y categorizaci√≥n
- Validaci√≥n de calidad de datos
- C√°lculo de m√©tricas de completitud

### 3. Carga (Load)
- Generaci√≥n de dataset en m√∫ltiples formatos
- Creaci√≥n de reportes de calidad
- Metadatos del dataset
- Validaci√≥n final

## Decisiones de Dise√±o

### Tecnolog√≠as Elegidas
- **Airflow**: Orquestaci√≥n y scheduling
- **pandas**: Manipulaci√≥n de datos
- **BeautifulSoup**: Web scraping
- **Parquet**: Almacenamiento eficiente

### Calidad de Datos
- Score de calidad por registro (0-100)
- Validaciones de completitud
- Detecci√≥n de datos an√≥malos
- Reportes autom√°ticos de calidad

### Formatos de Salida
- **Excel**: Para an√°lisis manual
- **CSV**: Compatibilidad universal
- **Parquet**: Eficiencia en big data
- **JSON**: Integraci√≥n con APIs
- **Metadata**: Documentaci√≥n autom√°tica

## Monitoreo y Alertas
- Logs detallados en cada etapa
- M√©tricas de calidad autom√°ticas
- Validaciones de integridad
- Alertas en caso de fallas

## Uso del Dataset
El dataset generado puede ser utilizado para:
- An√°lisis de precios inmobiliarios
- Estudios de mercado
- Machine Learning predictivo
- Dashboards y visualizaciones
- Research acad√©mico
"""

# ================================================================
# DEFINICI√ìN DE TASKS
# ================================================================

# Task inicial - Setup
start_task = DummyOperator(
    task_id='start_pipeline',
    dag=dag,
)

# Task de preparaci√≥n del entorno
prepare_environment = BashOperator(
    task_id='prepare_environment',
    bash_command="""
    echo "üöÄ Iniciando pipeline ETL de propiedades"
    echo "üìÖ Fecha: {{ ds }}"
    echo "‚è∞ Hora: {{ ts }}"
    echo "üîß Preparando directorios..."
    mkdir -p /tmp/data
    mkdir -p /tmp/logs
    echo "‚úÖ Entorno preparado"
    """,
    dag=dag,
)

# Task 1: Extracci√≥n de datos
extract_task = PythonOperator(
    task_id='extract_properties',
    python_callable=extract_properties_data,
    params=SCRAPING_CONFIG,
    dag=dag,
)

# Task 2: Transformaci√≥n de datos
transform_task = PythonOperator(
    task_id='transform_properties',
    python_callable=transform_properties_data,
    dag=dag,
)

# Task 3: Carga del dataset
load_task = PythonOperator(
    task_id='load_dataset',
    python_callable=load_final_dataset,
    dag=dag,
)

# Task 4: Validaci√≥n final
validate_task = PythonOperator(
    task_id='validate_dataset',
    python_callable=validate_final_dataset,
    dag=dag,
)

# Task de generaci√≥n de reporte final
generate_report = BashOperator(
    task_id='generate_final_report',
    bash_command="""
    echo "üìä REPORTE FINAL DEL PIPELINE"
    echo "================================"
    echo "üìÖ Fecha de ejecuci√≥n: {{ ds }}"
    echo "‚è∞ Hora de finalizaci√≥n: {{ ts }}"
    echo "‚úÖ Pipeline ejecutado exitosamente"
    echo "üìÅ Archivos generados en: /tmp/data/"
    echo "üìã Logs disponibles en: /tmp/logs/"
    echo "üéâ Dataset de propiedades listo para uso"
    
    # Listar archivos generados
    echo ""
    echo "üìÅ Archivos generados:"
    ls -la /tmp/data/ | grep propiedades || echo "No se encontraron archivos"
    
    # Mostrar tama√±o total
    echo ""
    echo "üíæ Espacio utilizado:"
    du -sh /tmp/data/ 2>/dev/null || echo "Directorio no encontrado"
    """,
    dag=dag,
)

# Task final
end_task = DummyOperator(
    task_id='end_pipeline',
    dag=dag,
)

# ================================================================
# DEFINICI√ìN DE DEPENDENCIAS
# ================================================================

# Configurar la secuencia de ejecuci√≥n
start_task >> prepare_environment >> extract_task >> transform_task >> load_task >> validate_task >> generate_report >> end_task

# ================================================================
# DOCUMENTACI√ìN DE TASKS
# ================================================================

extract_task.doc_md = """
## Task: Extracci√≥n de Propiedades

Esta task realiza el web scraping de ArgentProp para extraer datos de propiedades.

### Par√°metros de Configuraci√≥n:
- **property_type**: Tipo de propiedad (departamentos, casas)
- **operation_type**: Tipo de operaci√≥n (venta, alquiler)
- **location**: Ubicaci√≥n geogr√°fica
- **price_range**: Rango de precios a filtrar
- **max_pages**: N√∫mero m√°ximo de p√°ginas a scrapear

### Datos Extra√≠dos:
- T√≠tulo de la propiedad
- Precio publicado
- Ubicaci√≥n detallada
- Caracter√≠sticas (habitaciones, ba√±os, superficie)
- URL de la propiedad
- Imagen principal
- Fecha de scraping

### Salida:
Archivo JSON con datos crudos en `/tmp/raw_properties_TIMESTAMP.json`
"""

transform_task.doc_md = """
## Task: Transformaci√≥n de Datos

Esta task limpia y transforma los datos crudos extra√≠dos.

### Transformaciones Aplicadas:
- **Precios**: Normalizaci√≥n de formatos monetarios
- **Ubicaciones**: Parseo de barrio, zona y ciudad
- **Caracter√≠sticas**: Extracci√≥n de n√∫meros de habitaciones y ba√±os
- **Superficie**: Conversi√≥n a metros cuadrados num√©ricos
- **Calidad**: C√°lculo de score de calidad por registro

### Validaciones:
- Detecci√≥n de precios v√°lidos
- Validaci√≥n de ubicaciones
- Completitud de caracter√≠sticas
- Score de calidad (0-100)

### Salida:
Archivo Parquet con datos transformados en `/tmp/transformed_properties_TIMESTAMP.parquet`
"""

load_task.doc_md = """
## Task: Carga del Dataset

Esta task genera el dataset final en m√∫ltiples formatos.

### Formatos Generados:
- **Excel**: Incluye hojas de estad√≠sticas y calidad
- **CSV**: Formato universal para an√°lisis
- **Parquet**: Formato eficiente para big data
- **JSON**: Para integraci√≥n con APIs
- **Metadata**: Documentaci√≥n del dataset

### Reportes Incluidos:
- Estad√≠sticas descriptivas
- Distribuci√≥n por barrios
- An√°lisis de precios
- M√©tricas de calidad

### Salida:
M√∫ltiples archivos en `/tmp/data/propiedades_dataset_TIMESTAMP.*`
"""

validate_task.doc_md = """
## Task: Validaci√≥n del Dataset

Esta task valida la calidad e integridad del dataset final.

### Validaciones Realizadas:
- Presencia de registros
- Calidad m√≠nima de datos
- Existencia de archivos
- Completitud de campos cr√≠ticos

### M√©tricas Evaluadas:
- Total de propiedades
- Porcentaje con precios v√°lidos
- Diversidad de ubicaciones
- Score promedio de calidad

### Criterios de √âxito:
- M√≠nimo 1 registro extra√≠do
- Al menos 30% de calidad promedio
- Presencia de precios y ubicaciones
- Todos los archivos generados correctamente
"""
